{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "First we import all the libraries we want for this method, and import the training and test csv's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# text preprocessing modules\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# naive bayes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "#LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# import the training and test dataframes\n",
    "train_df = pd.read_csv('./Training-dataset.csv')\n",
    "validation_df = pd.read_csv('./Task-2-validation-dataset.csv')\n",
    "test_df = pd.read_csv('./Task-2-test-dataset1.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just do some analysis to look at the balance of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of movies = 8257\n",
      "Total number of movies without label = 0\n",
      "Total labels = 16193\n",
      "comedy        1262\n",
      "cult          1801\n",
      "flashback     1994\n",
      "historical     186\n",
      "murder        4019\n",
      "revenge       1657\n",
      "romantic      2006\n",
      "scifi          204\n",
      "violence      3064\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Returns a series of all the labels and how many synopses are classified by that label\n",
    "label_count = train_df.iloc[:,3:].sum()\n",
    "# For each synopsis return the number of labels assigned for that synopsis\n",
    "movie_label_count = train_df.iloc[:,3:].sum(axis=1) \n",
    "\n",
    "# Iterate through all our synopses, count up any non-labeled synopses\n",
    "no_label_count = 0\n",
    "for sum in movie_label_count.items():\n",
    "    if sum==0:\n",
    "        no_label_count +=1\n",
    "\n",
    "print(\"Total number of movies =\",len(train_df))\n",
    "print(\"Total number of movies without label =\",no_label_count)\n",
    "print(\"Total labels =\",label_count.sum())\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The same function will be used by both LSTM & Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    # for sentence in sent_tokenize(synopsis):\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
    "    # Tokenise the sentence\n",
    "    text = word_tokenize(text)\n",
    "    # get a set of the stopwords to remove\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Remove stopwords\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['title'] + ' ' + train_df['plot_synopsis']\n",
    "train_df.drop(columns=['title','plot_synopsis'], inplace=True)\n",
    "train_df['text'] = train_df['text'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df['text'] = validation_df['title'] + ' ' + validation_df['plot_synopsis']\n",
    "validation_df.drop(columns=['title','plot_synopsis'], inplace=True)\n",
    "validation_df['text'] = validation_df['text'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text'] = test_df['title'] + ' ' + test_df['plot_synopsis']\n",
    "test_df.drop(columns=['title','plot_synopsis'], inplace=True)\n",
    "test_df['text'] = test_df['text'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']\n"
     ]
    }
   ],
   "source": [
    "genres = list(train_df.iloc[:,1:10].columns)\n",
    "print(genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method (a) - Naive Bayes\n",
    "First let's create a bow vectorizer and use that to prepare the input & output data for the training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8257, 42514)\n",
      "(1188, 42514)\n",
      "(1200, 42514)\n",
      "(8257,)\n",
      "(1188,)\n"
     ]
    }
   ],
   "source": [
    "countvectorizer = CountVectorizer(min_df=2, max_df=0.8)\n",
    "X_train_nb = countvectorizer.fit_transform(train_df['text'])\n",
    "X_validation_nb = countvectorizer.transform(validation_df['text'])\n",
    "X_test_nb = countvectorizer.transform(test_df['text'])\n",
    "print(X_train_nb.shape)\n",
    "print(X_validation_nb.shape)\n",
    "print(X_test_nb.shape)\n",
    "\n",
    "Y_train_nb = []\n",
    "Y_validation_nb = []\n",
    "for genre in genres:\n",
    "    Y_train_nb.append(train_df[genre].to_numpy())\n",
    "    Y_validation_nb.append(validation_df[genre].to_numpy())\n",
    "print(Y_train_nb[0].shape)\n",
    "print(Y_validation_nb[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a Naive Bayes model for each genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training & predicting with comedy model . . .\n",
      "Training & predicting with cult model . . .\n",
      "Training & predicting with flashback model . . .\n",
      "Training & predicting with historical model . . .\n",
      "Training & predicting with murder model . . .\n",
      "Training & predicting with revenge model . . .\n",
      "Training & predicting with romantic model . . .\n",
      "Training & predicting with scifi model . . .\n",
      "Training & predicting with violence model . . .\n",
      "0.12770509719848633\n",
      "0.024897098541259766\n",
      "0.024585485458374023\n"
     ]
    }
   ],
   "source": [
    "models_nb = []\n",
    "predictions_val_nb = []\n",
    "predictions_test_nb = []\n",
    "\n",
    "train_time = 0\n",
    "classify_time = 0\n",
    "test_time = 0\n",
    "for i, genre in enumerate(genres):\n",
    "    print('Training & predicting with ' + genre + ' model . . .')\n",
    "\n",
    "    # Train the classifier\n",
    "    genre_model = MultinomialNB()\n",
    "    start = time.time()\n",
    "    genre_model.fit(X_train_nb, Y_train_nb[i])\n",
    "    end = time.time()\n",
    "    train_time += (end-start)\n",
    "\n",
    "    # Predict validation data\n",
    "    start = time.time()\n",
    "    pred_val = genre_model.predict(X_validation_nb)\n",
    "    end = time.time()\n",
    "    classify_time += (end-start)\n",
    "\n",
    "    start = time.time()\n",
    "    pred_test = genre_model.predict(X_test_nb)\n",
    "    end = time.time()\n",
    "    test_time += (end-start)\n",
    "\n",
    "    models_nb.append(genre_model)\n",
    "    predictions_val_nb.append(pred_val)\n",
    "    predictions_test_nb.append(pred_test)\n",
    "\n",
    "print(train_time)\n",
    "print(classify_time)\n",
    "print(test_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predictions for both validation and test dfs and do some final data manipulation so it's output in the correct format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_val_nb = np.transpose(np.array(predictions_val_nb))\n",
    "id_val_nb = validation_df['ID'].to_numpy().reshape(predictions_val_nb.shape[0], 1)\n",
    "all_val_nb = np.hstack((id_val_nb, predictions_val_nb))\n",
    "val_nb_df = pd.DataFrame(all_val_nb)\n",
    "\n",
    "predictions_test_nb = np.transpose(np.array(predictions_test_nb))\n",
    "id_test_nb = test_df['ID'].to_numpy().reshape(predictions_test_nb.shape[0], 1)\n",
    "all_test_nb = np.hstack((id_test_nb, predictions_test_nb))\n",
    "test_nb_df = pd.DataFrame(all_test_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         0  1  2  3  4  5  6  7  8  9\n",
      "0     cf32cb00-172d-40f2-a3c1-936e8a0d89d7  0  0  0  0  0  0  1  0  0\n",
      "1     df7e125e-2d59-40e4-a126-9397e3a0ef21  0  0  0  0  1  0  0  0  1\n",
      "2     49bc73f3-9179-41cd-9774-905c7a3ac91b  0  0  1  0  0  0  1  0  0\n",
      "3     0ed4822b-87af-44bc-a677-7f7abfdaccf3  0  0  0  0  0  1  1  0  0\n",
      "4     0b1b0fa4-43bc-41ba-9598-b3401894b96d  1  0  0  0  1  1  0  0  0\n",
      "...                                    ... .. .. .. .. .. .. .. .. ..\n",
      "1183  d32be875-41c7-4e84-ac04-e1d3bc3df0fe  0  0  1  0  0  0  1  0  0\n",
      "1184  84e025dd-4b4e-403c-a3dd-34818b210857  0  0  0  0  1  0  0  0  0\n",
      "1185  3d291d3b-c0b5-47cc-8dc8-127dc93162e3  0  1  0  0  0  0  0  0  1\n",
      "1186  6c9b3034-56b3-42f6-874e-a821c9fd1a89  0  0  1  0  0  0  1  0  0\n",
      "1187  fbd1d334-e979-465c-9fb0-e173d2642630  0  1  1  0  0  1  0  0  1\n",
      "\n",
      "[1188 rows x 10 columns]\n",
      "                                         0  1  2  3  4  5  6  7  8  9\n",
      "0     9484ac61-0e30-4799-9998-6f74f4cbb204  0  1  1  0  0  0  0  0  1\n",
      "1     55942d28-b6a2-4662-ab55-a66783a86a56  0  0  0  0  0  0  1  0  0\n",
      "2     b71ed317-04cd-42f5-a380-d21dfea2bd36  0  0  1  0  0  0  1  0  0\n",
      "3     5689b1b2-88cd-4c22-9114-0850ba539280  1  0  1  0  1  0  0  0  0\n",
      "4     a0d9062e-f539-4043-bc9e-2a2ed589477b  0  0  0  0  1  0  0  0  0\n",
      "...                                    ... .. .. .. .. .. .. .. .. ..\n",
      "1195  8978047a-ec54-412a-bcee-070fe1fb055c  0  1  0  0  0  0  0  0  1\n",
      "1196  f1f04933-e298-4f65-bbeb-bc61a567a688  1  0  0  0  0  0  0  0  0\n",
      "1197  a033955d-12c2-4549-bafd-ca8e84615f1b  1  1  0  0  0  0  0  0  0\n",
      "1198  9464e84d-36b6-4b69-b0fb-f3c0546a8b10  0  0  0  0  0  0  1  0  0\n",
      "1199  93ec8a32-0f64-4965-ba02-5b369ed16ca4  0  0  0  0  0  0  0  0  1\n",
      "\n",
      "[1200 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "filepath = Path('./10861383-Task2-method-a-validation.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "print(val_nb_df)\n",
    "val_nb_df.to_csv(filepath, index=False, header=False)  \n",
    "\n",
    "filepath = Path('./10861383-Task2-method-a.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "print(test_nb_df)\n",
    "test_nb_df.to_csv(filepath, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method (b) - LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the texts and labels for the train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8257 1188 1200\n",
      "(8257, 9)\n",
      "(1188, 9)\n"
     ]
    }
   ],
   "source": [
    "texts_train = list(train_df['text'])\n",
    "texts_validation = list(validation_df['text'])\n",
    "texts_test = list(test_df['text'])\n",
    "print(len(texts_train), len(texts_validation), len(texts_test))\n",
    "\n",
    "labels_train = []\n",
    "labels_validation = []\n",
    "for genre in genres:\n",
    "    labels_train.append(train_df[genre].to_numpy())\n",
    "    labels_validation.append(validation_df[genre].to_numpy())\n",
    "\n",
    "labels_train = np.flip(np.rot90(np.array(labels_train)), axis=0)\n",
    "labels_validation = np.flip(np.rot90(np.array(labels_validation)), axis=0)\n",
    "print(labels_train.shape)\n",
    "print(labels_validation.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the optimal hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "max_words = 10000 # max number of words to use in the vocabulary\n",
    "# max_len = max(train_df['text'].apply(max_length))\n",
    "max_len = 200  # max length of each text (in terms of number of words)\n",
    "embedding_dim = 100 # dimension of word embeddings\n",
    "lstm_units = 64 # number of units in the LSTM layer\n",
    "num_classes = len(genres) # number of classes\n",
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the texts and create a vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "train_sequences = tokenizer.texts_to_sequences(texts_train)\n",
    "val_sequences = tokenizer.texts_to_sequences(texts_validation)\n",
    "test_sequences = tokenizer.texts_to_sequences(texts_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8257, 200)\n",
      "(1188, 200)\n",
      "(1200, 200)\n",
      "(8257, 9)\n",
      "(1188, 9)\n"
     ]
    }
   ],
   "source": [
    "# Pad the sequences so they all have the same length\n",
    "X_train = pad_sequences(train_sequences, maxlen=max_len)\n",
    "X_val = pad_sequences(val_sequences, maxlen=max_len)\n",
    "X_test = pad_sequences(test_sequences, maxlen=max_len)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "Y_train = labels_train\n",
    "Y_val = labels_validation\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 200, 100)          1000000   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 200, 64)           42240     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 200, 64)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 9)                 585       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1075849 (4.10 MB)\n",
      "Trainable params: 1075849 (4.10 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "model.add(LSTM(lstm_units, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(lstm_units))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259/259 [==============================] - 30s 101ms/step - loss: 0.4905 - val_loss: 0.4540\n",
      "Epoch 2/10\n",
      "259/259 [==============================] - 25s 97ms/step - loss: 0.4411 - val_loss: 0.4252\n",
      "Epoch 3/10\n",
      "259/259 [==============================] - 24s 94ms/step - loss: 0.4006 - val_loss: 0.4319\n",
      "Epoch 4/10\n",
      "259/259 [==============================] - 24s 94ms/step - loss: 0.3608 - val_loss: 0.4524\n",
      "Epoch 5/10\n",
      "259/259 [==============================] - 25s 97ms/step - loss: 0.3214 - val_loss: 0.4758\n",
      "Epoch 6/10\n",
      "259/259 [==============================] - 25s 96ms/step - loss: 0.2882 - val_loss: 0.5057\n",
      "Epoch 7/10\n",
      "259/259 [==============================] - 25s 96ms/step - loss: 0.2604 - val_loss: 0.5450\n",
      "Epoch 8/10\n",
      "259/259 [==============================] - 26s 100ms/step - loss: 0.2392 - val_loss: 0.5934\n",
      "Epoch 9/10\n",
      "259/259 [==============================] - 26s 100ms/step - loss: 0.2164 - val_loss: 0.6233\n",
      "Epoch 10/10\n",
      "259/259 [==============================] - 25s 96ms/step - loss: 0.1983 - val_loss: 0.6686\n",
      "255.8164303302765\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "start = time.time()\n",
    "model.fit(  X_train, \n",
    "            Y_train, \n",
    "            validation_data=(\n",
    "                X_val, \n",
    "                Y_val),\n",
    "            batch_size=batch_size, \n",
    "            epochs=10,\n",
    "            )\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict the validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 2s 42ms/step\n",
      "4.366817474365234\n",
      "38/38 [==============================] - 2s 41ms/step\n",
      "1.6558811664581299\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pred_val = model.predict(X_val)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "start = time.time()\n",
    "pred_test = model.predict(X_test)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1188, 9)\n",
      "(1200, 9)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(genres)):\n",
    "    pred_val[pred_val>=0.1]=1\n",
    "    pred_val[pred_val<0.1]=0\n",
    "    pred_test[pred_test>=0.1]=1\n",
    "    pred_test[pred_test<0.1]=0\n",
    "print(pred_val.shape)\n",
    "print(pred_test.shape)\n",
    "\n",
    "pred_val = pred_val.astype(int) \n",
    "pred_test = pred_test.astype(int) \n",
    "\n",
    "id_val = validation_df['ID'].to_numpy().reshape(pred_val.shape[0], 1)\n",
    "id_test = test_df['ID'].to_numpy().reshape(pred_test.shape[0], 1)\n",
    "all_val = np.hstack((id_val, pred_val))\n",
    "all_test = np.hstack((id_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_output = pd.DataFrame(all_val)\n",
    "filepath = Path('./10861383-Task2-method-b-validation.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "val_df_output.to_csv(filepath, index=False, header=False)  \n",
    "\n",
    "test_df_output = pd.DataFrame(all_test)\n",
    "filepath = Path('./10861383-Task2-method-b.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "test_df_output.to_csv(filepath, index=False, header=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_output_shape_at(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
