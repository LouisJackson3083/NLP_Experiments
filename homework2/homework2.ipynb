{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize, ToktokTokenizer, wordpunct_tokenize, sent_tokenize, WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257058 ['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'Crime', 'and', 'Punishment', ',']\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "print(len(tokens), tokens[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: ï»¿The Project Gutenberg eBook of Crime and Punishment...>\n",
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; young man; Nikodim Fomitch; Project Gutenberg; Ilya\n",
      "Petrovitch; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
      "heavens\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print(text)\n",
    "print(text.collocations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5575 1171580\n"
     ]
    }
   ],
   "source": [
    "print(raw.find(\"PART I\"), raw.rfind(\"You\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The effect of case folding on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize non-case folded tokens: 257058\n",
      "word_tokenize case folded tokens: 257033\n",
      "\n",
      "ToktokTokenizer non-case folded tokens: 1217441\n",
      "ToktokTokenizer case folded tokens: 1217441\n",
      "\n",
      "wordpunct_tokenize non-case folded tokens: 255819\n",
      "wordpunct_tokenize case folded tokens: 255819\n",
      "\n",
      "sent_tokenize non-case folded tokens: 12060\n",
      "sent_tokenize case folded tokens: 11726\n",
      "\n",
      "WhitespaceTokenizer non-case folded tokens: 206551\n",
      "WhitespaceTokenizer case folded tokens: 206551\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_lowered = raw.lower()\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "print(\"word_tokenize non-case folded tokens: \" + str(len(tokens)))\n",
    "tokens = word_tokenize(raw_lowered)\n",
    "print(\"word_tokenize case folded tokens: \" + str(len(tokens)))\n",
    "print()\n",
    "\n",
    "toktok = ToktokTokenizer()\n",
    "tokens = toktok.tokenize(text=raw, return_str=True)\n",
    "print(\"ToktokTokenizer non-case folded tokens: \" + str(len(tokens)))\n",
    "tokens = toktok.tokenize(text=raw_lowered, return_str=True)\n",
    "print(\"ToktokTokenizer case folded tokens: \" + str(len(tokens)))\n",
    "print()\n",
    "\n",
    "tokens = wordpunct_tokenize(raw)\n",
    "print(\"wordpunct_tokenize non-case folded tokens: \" + str(len(tokens)))\n",
    "tokens = wordpunct_tokenize(raw_lowered)\n",
    "print(\"wordpunct_tokenize case folded tokens: \" + str(len(tokens)))\n",
    "print()\n",
    "\n",
    "tokens = sent_tokenize(raw)\n",
    "print(\"sent_tokenize non-case folded tokens: \" + str(len(tokens)))\n",
    "tokens = sent_tokenize(raw_lowered)\n",
    "print(\"sent_tokenize case folded tokens: \" + str(len(tokens)))\n",
    "print()\n",
    "\n",
    "tokens = list(WhitespaceTokenizer().span_tokenize(raw))\n",
    "print(\"WhitespaceTokenizer non-case folded tokens: \" + str(len(tokens)))\n",
    "tokens = list(WhitespaceTokenizer().span_tokenize(raw_lowered))\n",
    "print(\"WhitespaceTokenizer case folded tokens: \" + str(len(tokens)))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming & lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize token amount: 11516\n",
      "word_tokenize token set amount: 11516\n",
      "porter stemmer token amount: 11516\n",
      "lancaster stemmer token amount: 11516\n",
      "\n",
      "ToktokTokenizer token amount: 1217441\n",
      "ToktokTokenizer token set amount: 101\n",
      "porter stemmer token amount: 1217441\n",
      "lancaster stemmer token amount: 1217441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "tokens = set(word_tokenize(raw))\n",
    "tokens_set = set(tokens)\n",
    "print(\"word_tokenize token amount: \" + str(len(tokens)))\n",
    "print(\"word_tokenize token set amount: \" + str(len(tokens_set)))\n",
    "stemmed_tokens = [porter.stem(t) for t in tokens]\n",
    "print(\"porter stemmer token amount: \" + str(len(stemmed_tokens)))\n",
    "stemmed_tokens = [lancaster.stem(t) for t in tokens]\n",
    "print(\"lancaster stemmer token amount: \" + str(len(stemmed_tokens)))\n",
    "print()\n",
    "\n",
    "toktok = ToktokTokenizer()\n",
    "tokens = toktok.tokenize(text=raw, return_str=True)\n",
    "tokens_set = set(tokens)\n",
    "print(\"ToktokTokenizer token amount: \" + str(len(tokens)))\n",
    "print(\"ToktokTokenizer token set amount: \" + str(len(tokens_set)))\n",
    "stemmed_tokens = [porter.stem(t) for t in tokens]\n",
    "print(\"porter stemmer token amount: \" + str(len(stemmed_tokens)))\n",
    "stemmed_tokens = [lancaster.stem(t) for t in tokens]\n",
    "print(\"lancaster stemmer token amount: \" + str(len(stemmed_tokens)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org.\n",
      "If you are not located in the United States, you\n",
      "will have to check the laws of the country where you are located before\n",
      "using this eBook.\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(raw)\n",
    "print(sentences[1])\n",
    "print(sentences[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\ufeffThe',)\n",
      "('Project',)\n",
      "('Gutenberg',)\n",
      "('eBook',)\n",
      "('of',)\n",
      "('Crime',)\n",
      "('and',)\n",
      "('Punishment,',)\n",
      "('by',)\n",
      "('Fyodor',)\n",
      "('Dostoevsky',)\n",
      "('This',)\n",
      "('eBook',)\n",
      "('is',)\n",
      "('for',)\n",
      "('the',)\n",
      "('use',)\n",
      "('of',)\n",
      "('anyone',)\n",
      "('anywhere',)\n",
      "('in',)\n",
      "('the',)\n",
      "('United',)\n",
      "('States',)\n",
      "('and',)\n",
      "('most',)\n",
      "('other',)\n",
      "('parts',)\n",
      "('of',)\n",
      "('the',)\n",
      "('world',)\n",
      "('at',)\n",
      "('no',)\n",
      "('cost',)\n",
      "('and',)\n",
      "('with',)\n",
      "('almost',)\n",
      "('no',)\n",
      "('restrictions',)\n",
      "('whatsoever.',)\n",
      "('You',)\n",
      "('may',)\n",
      "('copy',)\n",
      "('it,',)\n",
      "('give',)\n",
      "('it',)\n",
      "('away',)\n",
      "('or',)\n",
      "('re-use',)\n",
      "('it',)\n",
      "('under',)\n",
      "('the',)\n",
      "('terms',)\n",
      "('of',)\n",
      "('the',)\n",
      "('Project',)\n",
      "('Gutenberg',)\n",
      "('License',)\n",
      "('included',)\n",
      "('with',)\n",
      "('this',)\n",
      "('eBook',)\n",
      "('or',)\n",
      "('online',)\n",
      "('at',)\n",
      "('www.gutenberg.org.',)\n",
      "('If',)\n",
      "('you',)\n",
      "('are',)\n",
      "('not',)\n",
      "('located',)\n",
      "('in',)\n",
      "('the',)\n",
      "('United',)\n",
      "('States,',)\n",
      "('you',)\n",
      "('will',)\n",
      "('have',)\n",
      "('to',)\n",
      "('check',)\n",
      "('the',)\n",
      "('laws',)\n",
      "('of',)\n",
      "('the',)\n",
      "('country',)\n",
      "('where',)\n",
      "('you',)\n",
      "('are',)\n",
      "('located',)\n",
      "('before',)\n",
      "('using',)\n",
      "('this',)\n",
      "('eBook.',)\n",
      "('Title:',)\n",
      "('Crime',)\n",
      "('and',)\n",
      "('Punishment',)\n",
      "('Author:',)\n",
      "('Fyodor',)\n",
      "('Dostoevsky',)\n",
      "('Translator:',)\n",
      "('Constance',)\n",
      "('Garnett',)\n",
      "('Release',)\n",
      "('Date:',)\n",
      "('March,',)\n",
      "('2001',)\n",
      "('[eBook',)\n",
      "('#2554]',)\n",
      "('[Most',)\n",
      "('recently',)\n",
      "('updated:',)\n",
      "('August',)\n",
      "('6,',)\n",
      "('2021]',)\n",
      "('Language:',)\n",
      "('English',)\n",
      "('Character',)\n",
      "('set',)\n",
      "('encoding:',)\n",
      "('UTF-8',)\n",
      "('Produced',)\n",
      "('by:',)\n",
      "('John',)\n",
      "('Bickers,',)\n",
      "('Dagny',)\n",
      "('and',)\n",
      "('David',)\n",
      "('Widger',)\n",
      "('***',)\n",
      "('START',)\n",
      "('OF',)\n",
      "('THE',)\n",
      "('PROJECT',)\n",
      "('GUTENBERG',)\n",
      "('EBOOK',)\n",
      "('CRIME',)\n",
      "('AND',)\n",
      "('PUNISHMENT',)\n",
      "('***',)\n",
      "('CRIME',)\n",
      "('AND',)\n",
      "('PUNISHMENT',)\n",
      "('By',)\n",
      "('Fyodor',)\n",
      "('Dostoevsky',)\n",
      "('Translated',)\n",
      "('By',)\n",
      "('Constance',)\n",
      "('Garnett',)\n",
      "('TRANSLATORâS',)\n",
      "('PREFA',)\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "unigrams = ngrams(raw[:1000].split(), 1)\n",
    "for item in unigrams:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\ufeffThe', 'Project')\n",
      "('Project', 'Gutenberg')\n",
      "('Gutenberg', 'eBook')\n",
      "('eBook', 'of')\n",
      "('of', 'Crime')\n",
      "('Crime', 'and')\n",
      "('and', 'Punishment,')\n",
      "('Punishment,', 'by')\n",
      "('by', 'Fyodor')\n",
      "('Fyodor', 'Dostoevsky')\n",
      "('Dostoevsky', 'This')\n",
      "('This', 'eBook')\n",
      "('eBook', 'is')\n",
      "('is', 'for')\n",
      "('for', 'the')\n",
      "('the', 'use')\n",
      "('use', 'of')\n",
      "('of', 'anyone')\n",
      "('anyone', 'anywhere')\n",
      "('anywhere', 'in')\n",
      "('in', 'the')\n",
      "('the', 'United')\n",
      "('United', 'States')\n",
      "('States', 'and')\n",
      "('and', 'most')\n",
      "('most', 'other')\n",
      "('other', 'parts')\n",
      "('parts', 'of')\n",
      "('of', 'the')\n",
      "('the', 'world')\n",
      "('world', 'at')\n",
      "('at', 'no')\n",
      "('no', 'cost')\n",
      "('cost', 'and')\n",
      "('and', 'with')\n",
      "('with', 'almost')\n",
      "('almost', 'no')\n",
      "('no', 'restrictions')\n",
      "('restrictions', 'whatsoever.')\n",
      "('whatsoever.', 'You')\n",
      "('You', 'may')\n",
      "('may', 'copy')\n",
      "('copy', 'it,')\n",
      "('it,', 'give')\n",
      "('give', 'it')\n",
      "('it', 'away')\n",
      "('away', 'or')\n",
      "('or', 're-use')\n",
      "('re-use', 'it')\n",
      "('it', 'under')\n",
      "('under', 'the')\n",
      "('the', 'terms')\n",
      "('terms', 'of')\n",
      "('of', 'the')\n",
      "('the', 'Project')\n",
      "('Project', 'Gutenberg')\n",
      "('Gutenberg', 'License')\n",
      "('License', 'included')\n",
      "('included', 'with')\n",
      "('with', 'this')\n",
      "('this', 'eBook')\n",
      "('eBook', 'or')\n",
      "('or', 'online')\n",
      "('online', 'at')\n",
      "('at', 'www.gutenberg.org.')\n",
      "('www.gutenberg.org.', 'If')\n",
      "('If', 'you')\n",
      "('you', 'are')\n",
      "('are', 'not')\n",
      "('not', 'located')\n",
      "('located', 'in')\n",
      "('in', 'the')\n",
      "('the', 'United')\n",
      "('United', 'States,')\n",
      "('States,', 'you')\n",
      "('you', 'will')\n",
      "('will', 'have')\n",
      "('have', 'to')\n",
      "('to', 'check')\n",
      "('check', 'the')\n",
      "('the', 'laws')\n",
      "('laws', 'of')\n",
      "('of', 'the')\n",
      "('the', 'country')\n",
      "('country', 'where')\n",
      "('where', 'you')\n",
      "('you', 'are')\n",
      "('are', 'located')\n",
      "('located', 'before')\n",
      "('before', 'using')\n",
      "('using', 'this')\n",
      "('this', 'eBook.')\n",
      "('eBook.', 'Title:')\n",
      "('Title:', 'Crime')\n",
      "('Crime', 'and')\n",
      "('and', 'Punishment')\n",
      "('Punishment', 'Author:')\n",
      "('Author:', 'Fyodor')\n",
      "('Fyodor', 'Dostoevsky')\n",
      "('Dostoevsky', 'Translator:')\n",
      "('Translator:', 'Constance')\n",
      "('Constance', 'Garnett')\n",
      "('Garnett', 'Release')\n",
      "('Release', 'Date:')\n",
      "('Date:', 'March,')\n",
      "('March,', '2001')\n",
      "('2001', '[eBook')\n",
      "('[eBook', '#2554]')\n",
      "('#2554]', '[Most')\n",
      "('[Most', 'recently')\n",
      "('recently', 'updated:')\n",
      "('updated:', 'August')\n",
      "('August', '6,')\n",
      "('6,', '2021]')\n",
      "('2021]', 'Language:')\n",
      "('Language:', 'English')\n",
      "('English', 'Character')\n",
      "('Character', 'set')\n",
      "('set', 'encoding:')\n",
      "('encoding:', 'UTF-8')\n",
      "('UTF-8', 'Produced')\n",
      "('Produced', 'by:')\n",
      "('by:', 'John')\n",
      "('John', 'Bickers,')\n",
      "('Bickers,', 'Dagny')\n",
      "('Dagny', 'and')\n",
      "('and', 'David')\n",
      "('David', 'Widger')\n",
      "('Widger', '***')\n",
      "('***', 'START')\n",
      "('START', 'OF')\n",
      "('OF', 'THE')\n",
      "('THE', 'PROJECT')\n",
      "('PROJECT', 'GUTENBERG')\n",
      "('GUTENBERG', 'EBOOK')\n",
      "('EBOOK', 'CRIME')\n",
      "('CRIME', 'AND')\n",
      "('AND', 'PUNISHMENT')\n",
      "('PUNISHMENT', '***')\n",
      "('***', 'CRIME')\n",
      "('CRIME', 'AND')\n",
      "('AND', 'PUNISHMENT')\n",
      "('PUNISHMENT', 'By')\n",
      "('By', 'Fyodor')\n",
      "('Fyodor', 'Dostoevsky')\n",
      "('Dostoevsky', 'Translated')\n",
      "('Translated', 'By')\n",
      "('By', 'Constance')\n",
      "('Constance', 'Garnett')\n",
      "('Garnett', 'TRANSLATORâS')\n",
      "('TRANSLATORâS', 'PREFA')\n"
     ]
    }
   ],
   "source": [
    "bigrams = ngrams(raw[:1000].split(), 2)\n",
    "for item in bigrams:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\ufeffThe', 'Project', 'Gutenberg')\n",
      "('Project', 'Gutenberg', 'eBook')\n",
      "('Gutenberg', 'eBook', 'of')\n",
      "('eBook', 'of', 'Crime')\n",
      "('of', 'Crime', 'and')\n",
      "('Crime', 'and', 'Punishment,')\n",
      "('and', 'Punishment,', 'by')\n",
      "('Punishment,', 'by', 'Fyodor')\n",
      "('by', 'Fyodor', 'Dostoevsky')\n",
      "('Fyodor', 'Dostoevsky', 'This')\n",
      "('Dostoevsky', 'This', 'eBook')\n",
      "('This', 'eBook', 'is')\n",
      "('eBook', 'is', 'for')\n",
      "('is', 'for', 'the')\n",
      "('for', 'the', 'use')\n",
      "('the', 'use', 'of')\n",
      "('use', 'of', 'anyone')\n",
      "('of', 'anyone', 'anywhere')\n",
      "('anyone', 'anywhere', 'in')\n",
      "('anywhere', 'in', 'the')\n",
      "('in', 'the', 'United')\n",
      "('the', 'United', 'States')\n",
      "('United', 'States', 'and')\n",
      "('States', 'and', 'most')\n",
      "('and', 'most', 'other')\n",
      "('most', 'other', 'parts')\n",
      "('other', 'parts', 'of')\n",
      "('parts', 'of', 'the')\n",
      "('of', 'the', 'world')\n",
      "('the', 'world', 'at')\n",
      "('world', 'at', 'no')\n",
      "('at', 'no', 'cost')\n",
      "('no', 'cost', 'and')\n",
      "('cost', 'and', 'with')\n",
      "('and', 'with', 'almost')\n",
      "('with', 'almost', 'no')\n",
      "('almost', 'no', 'restrictions')\n",
      "('no', 'restrictions', 'whatsoever.')\n",
      "('restrictions', 'whatsoever.', 'You')\n",
      "('whatsoever.', 'You', 'may')\n",
      "('You', 'may', 'copy')\n",
      "('may', 'copy', 'it,')\n",
      "('copy', 'it,', 'give')\n",
      "('it,', 'give', 'it')\n",
      "('give', 'it', 'away')\n",
      "('it', 'away', 'or')\n",
      "('away', 'or', 're-use')\n",
      "('or', 're-use', 'it')\n",
      "('re-use', 'it', 'under')\n",
      "('it', 'under', 'the')\n",
      "('under', 'the', 'terms')\n",
      "('the', 'terms', 'of')\n",
      "('terms', 'of', 'the')\n",
      "('of', 'the', 'Project')\n",
      "('the', 'Project', 'Gutenberg')\n",
      "('Project', 'Gutenberg', 'License')\n",
      "('Gutenberg', 'License', 'included')\n",
      "('License', 'included', 'with')\n",
      "('included', 'with', 'this')\n",
      "('with', 'this', 'eBook')\n",
      "('this', 'eBook', 'or')\n",
      "('eBook', 'or', 'online')\n",
      "('or', 'online', 'at')\n",
      "('online', 'at', 'www.gutenberg.org.')\n",
      "('at', 'www.gutenberg.org.', 'If')\n",
      "('www.gutenberg.org.', 'If', 'you')\n",
      "('If', 'you', 'are')\n",
      "('you', 'are', 'not')\n",
      "('are', 'not', 'located')\n",
      "('not', 'located', 'in')\n",
      "('located', 'in', 'the')\n",
      "('in', 'the', 'United')\n",
      "('the', 'United', 'States,')\n",
      "('United', 'States,', 'you')\n",
      "('States,', 'you', 'will')\n",
      "('you', 'will', 'have')\n",
      "('will', 'have', 'to')\n",
      "('have', 'to', 'check')\n",
      "('to', 'check', 'the')\n",
      "('check', 'the', 'laws')\n",
      "('the', 'laws', 'of')\n",
      "('laws', 'of', 'the')\n",
      "('of', 'the', 'country')\n",
      "('the', 'country', 'where')\n",
      "('country', 'where', 'you')\n",
      "('where', 'you', 'are')\n",
      "('you', 'are', 'located')\n",
      "('are', 'located', 'before')\n",
      "('located', 'before', 'using')\n",
      "('before', 'using', 'this')\n",
      "('using', 'this', 'eBook.')\n",
      "('this', 'eBook.', 'Title:')\n",
      "('eBook.', 'Title:', 'Crime')\n",
      "('Title:', 'Crime', 'and')\n",
      "('Crime', 'and', 'Punishment')\n",
      "('and', 'Punishment', 'Author:')\n",
      "('Punishment', 'Author:', 'Fyodor')\n",
      "('Author:', 'Fyodor', 'Dostoevsky')\n",
      "('Fyodor', 'Dostoevsky', 'Translator:')\n",
      "('Dostoevsky', 'Translator:', 'Constance')\n",
      "('Translator:', 'Constance', 'Garnett')\n",
      "('Constance', 'Garnett', 'Release')\n",
      "('Garnett', 'Release', 'Date:')\n",
      "('Release', 'Date:', 'March,')\n",
      "('Date:', 'March,', '2001')\n",
      "('March,', '2001', '[eBook')\n",
      "('2001', '[eBook', '#2554]')\n",
      "('[eBook', '#2554]', '[Most')\n",
      "('#2554]', '[Most', 'recently')\n",
      "('[Most', 'recently', 'updated:')\n",
      "('recently', 'updated:', 'August')\n",
      "('updated:', 'August', '6,')\n",
      "('August', '6,', '2021]')\n",
      "('6,', '2021]', 'Language:')\n",
      "('2021]', 'Language:', 'English')\n",
      "('Language:', 'English', 'Character')\n",
      "('English', 'Character', 'set')\n",
      "('Character', 'set', 'encoding:')\n",
      "('set', 'encoding:', 'UTF-8')\n",
      "('encoding:', 'UTF-8', 'Produced')\n",
      "('UTF-8', 'Produced', 'by:')\n",
      "('Produced', 'by:', 'John')\n",
      "('by:', 'John', 'Bickers,')\n",
      "('John', 'Bickers,', 'Dagny')\n",
      "('Bickers,', 'Dagny', 'and')\n",
      "('Dagny', 'and', 'David')\n",
      "('and', 'David', 'Widger')\n",
      "('David', 'Widger', '***')\n",
      "('Widger', '***', 'START')\n",
      "('***', 'START', 'OF')\n",
      "('START', 'OF', 'THE')\n",
      "('OF', 'THE', 'PROJECT')\n",
      "('THE', 'PROJECT', 'GUTENBERG')\n",
      "('PROJECT', 'GUTENBERG', 'EBOOK')\n",
      "('GUTENBERG', 'EBOOK', 'CRIME')\n",
      "('EBOOK', 'CRIME', 'AND')\n",
      "('CRIME', 'AND', 'PUNISHMENT')\n",
      "('AND', 'PUNISHMENT', '***')\n",
      "('PUNISHMENT', '***', 'CRIME')\n",
      "('***', 'CRIME', 'AND')\n",
      "('CRIME', 'AND', 'PUNISHMENT')\n",
      "('AND', 'PUNISHMENT', 'By')\n",
      "('PUNISHMENT', 'By', 'Fyodor')\n",
      "('By', 'Fyodor', 'Dostoevsky')\n",
      "('Fyodor', 'Dostoevsky', 'Translated')\n",
      "('Dostoevsky', 'Translated', 'By')\n",
      "('Translated', 'By', 'Constance')\n",
      "('By', 'Constance', 'Garnett')\n",
      "('Constance', 'Garnett', 'TRANSLATORâS')\n",
      "('Garnett', 'TRANSLATORâS', 'PREFA')\n"
     ]
    }
   ],
   "source": [
    "trigrams = ngrams(raw[:1000].split(), 3)\n",
    "for item in trigrams:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating tf...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\neoni\\OneDrive\\Documents\\UNI\\COMP34711_NLP\\homework2\\homework2.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neoni/OneDrive/Documents/UNI/COMP34711_NLP/homework2/homework2.ipynb#X20sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     final \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(doc_tf_idf\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m item: item[\u001b[39m1\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neoni/OneDrive/Documents/UNI/COMP34711_NLP/homework2/homework2.ipynb#X20sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m final[\u001b[39m-\u001b[39mk:]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/neoni/OneDrive/Documents/UNI/COMP34711_NLP/homework2/homework2.ipynb#X20sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mprint\u001b[39m(get_tfidf(\u001b[39m\"\u001b[39;49m\u001b[39myou\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m3\u001b[39;49m))\n",
      "\u001b[1;32mc:\\Users\\neoni\\OneDrive\\Documents\\UNI\\COMP34711_NLP\\homework2\\homework2.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neoni/OneDrive/Documents/UNI/COMP34711_NLP/homework2/homework2.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m fd \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mFreqDist()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neoni/OneDrive/Documents/UNI/COMP34711_NLP/homework2/homework2.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m text \u001b[39m=\u001b[39m gutenberg\u001b[39m.\u001b[39mraw(fileid)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/neoni/OneDrive/Documents/UNI/COMP34711_NLP/homework2/homework2.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m text \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neoni/OneDrive/Documents/UNI/COMP34711_NLP/homework2/homework2.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/neoni/OneDrive/Documents/UNI/COMP34711_NLP/homework2/homework2.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m found \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\neoni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\neoni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt/\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\neoni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   1278\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[39m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1281\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[1;32mc:\\Users\\neoni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;49;00m s, e \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\Users\\neoni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\Users\\neoni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[39mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1328\u001b[0m     slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1329\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m slices:\n\u001b[0;32m   1330\u001b[0m     \u001b[39myield\u001b[39;00m (sentence\u001b[39m.\u001b[39mstart, sentence\u001b[39m.\u001b[39mstop)\n",
      "File \u001b[1;32mc:\\Users\\neoni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \u001b[39mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \u001b[39mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[39m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m realign \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1459\u001b[0m \u001b[39mfor\u001b[39;00m sentence1, sentence2 \u001b[39min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1460\u001b[0m     sentence1 \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(sentence1\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m realign, sentence1\u001b[39m.\u001b[39mstop)\n\u001b[0;32m   1461\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32mc:\\Users\\neoni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:324\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 324\u001b[0m \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m iterator:\n\u001b[0;32m    325\u001b[0m     \u001b[39myield\u001b[39;00m (prev, el)\n\u001b[0;32m    326\u001b[0m     prev \u001b[39m=\u001b[39m el\n",
      "File \u001b[1;32mc:\\Users\\neoni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_slices_from_text\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mslice\u001b[39m]:\n\u001b[0;32m   1430\u001b[0m     last_break \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1431\u001b[0m     \u001b[39mfor\u001b[39;00m match, context \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[0;32m   1432\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1433\u001b[0m             \u001b[39myield\u001b[39;00m \u001b[39mslice\u001b[39m(last_break, match\u001b[39m.\u001b[39mend())\n",
      "File \u001b[1;32mc:\\Users\\neoni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1399\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1395\u001b[0m \u001b[39mfor\u001b[39;00m match \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang_vars\u001b[39m.\u001b[39mperiod_context_re()\u001b[39m.\u001b[39mfinditer(text):\n\u001b[0;32m   1396\u001b[0m \n\u001b[0;32m   1397\u001b[0m     \u001b[39m# Get the slice of the previous word\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m     before_text \u001b[39m=\u001b[39m text[previous_slice\u001b[39m.\u001b[39mstop : match\u001b[39m.\u001b[39mstart()]\n\u001b[1;32m-> 1399\u001b[0m     index_after_last_space \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_last_whitespace_index(before_text)\n\u001b[0;32m   1400\u001b[0m     \u001b[39mif\u001b[39;00m index_after_last_space:\n\u001b[0;32m   1401\u001b[0m         \u001b[39m# + 1 to exclude the space itself\u001b[39;00m\n\u001b[0;32m   1402\u001b[0m         index_after_last_space \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m previous_slice\u001b[39m.\u001b[39mstop \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\neoni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1350\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._get_last_whitespace_index\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1345\u001b[0m \u001b[39mGiven a text, find the index of the *last* occurrence of *any*\u001b[39;00m\n\u001b[0;32m   1346\u001b[0m \u001b[39mwhitespace character, i.e. \" \", \"\\n\", \"\\t\", \"\\r\", etc.\u001b[39;00m\n\u001b[0;32m   1347\u001b[0m \u001b[39mIf none is found, return 0.\u001b[39;00m\n\u001b[0;32m   1348\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1349\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(text) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m-> 1350\u001b[0m     \u001b[39mif\u001b[39;00m text[i] \u001b[39min\u001b[39;00m string\u001b[39m.\u001b[39mwhitespace:\n\u001b[0;32m   1351\u001b[0m         \u001b[39mreturn\u001b[39;00m i\n\u001b[0;32m   1352\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.text import Text\n",
    "import regex as re\n",
    "import math\n",
    "\n",
    "def get_tfidf(word, k):\n",
    "    fileids = gutenberg.fileids()\n",
    "    pattern = re.compile(r'\\b{}\\b'.format(word), re.I) # case insensitive\n",
    "    doc_count = 0\n",
    "    tf = []\n",
    "    \n",
    "    print(\"Calculating tf...\")\n",
    "    for fileid in fileids:\n",
    "        fd = nltk.FreqDist()\n",
    "        text = gutenberg.raw(fileid)\n",
    "        text = word_tokenize(text)\n",
    "        count = 0\n",
    "        found = False\n",
    "        for text_word in text:\n",
    "            if text_word == word:\n",
    "                count += 1\n",
    "                found = True\n",
    "        tf.append(count/len(text))\n",
    "        if (found):\n",
    "            doc_count += 1\n",
    "    \n",
    "    print(\"Calculating idf and sorting...\")\n",
    "\n",
    "    idf = math.log(len(fileids)/doc_count)\n",
    "    if idf == 0:\n",
    "        print(\"the term appears in every document, breaking\")\n",
    "        return None\n",
    "\n",
    "    doc_tf_idf = {}\n",
    "    for index, fileid in enumerate(fileids):\n",
    "        doc_tf_idf[fileid] = tf[index]/idf\n",
    "\n",
    "    final = sorted(doc_tf_idf.items(), key=lambda item: item[1])\n",
    "    return final[-k:]\n",
    "\n",
    "\n",
    "print(get_tfidf(\"tea\", 3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
